{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU20vB7XM3xC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file = json.load(open('json_extraction_dataset_500.json', 'r'))\n",
        "print(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "bJcGIwt7NNkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For GPU Check\n",
        "import torch\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ],
      "metadata": {
        "id": "SsbxDadkmcYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/tinyllama-chat-bnb-4bit\"\n",
        "\n",
        "max_seq_length = 2048  # choose sequence legth\n",
        "dtype = None  # Auto detection\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")"
      ],
      "metadata": {
        "id": "QwTSnJGrnW6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "from datasets import Dataset\n",
        "\n",
        "def format_prompt(example):\n",
        "  prompt = f\"\"\"###<|system|> You are a helpful AI assistant.</s> ### <|user|> {example['input']}</s> ### <|assistant|> {json.dumps(example['output'])}\"\"\"\n",
        "  return prompt\n",
        "\n",
        "formatted_data = [format_prompt(item) for item in file]\n",
        "dataset = Dataset.from_dict({\"text\":formatted_data})"
      ],
      "metadata": {
        "id": "3GSQ2fVIpuAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA Adaptors\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model=model,\n",
        "    r=64, # LoRA rank - higher = more capacity, more memory\n",
        "    target_modules = [\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",     # Supports any, but = \"none\"is optimized\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False, # Rank stabilized LoRA\n",
        "    loftq_config=None, #LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "q82fm476rnkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning the model\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Training arguments optimized for unsloth\n",
        "trainer = SFTTrainer( # Supervised Fine-tuning Trainer\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_text_field=\"text\",\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=25,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"output\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        dataloader_pin_memory=False,\n",
        "        report_to=\"none\",  # Disable wandb logging\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "EgeUCDz__LeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "8XSo-RA2CoB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fine-tuned model\n",
        "FastLanguageModel.for_inference(model) # Enable 2x faster inference\n",
        "\n",
        "# Test prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Extract the product information:\\n<div class='product'><h2>iPad Air</h2><span class='price'>$1344</span><span class='category'>audio</span><span class='brand'>Dell</span></div>\"}\n",
        "]\n",
        "\n",
        "# Set the chat template if it's not already set\n",
        "# if tokenizer.chat_template is None:\n",
        "#     tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'system' %}{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\n",
        "\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "id": "jfq2Ng75D5-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a GGUF model file\n",
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
      ],
      "metadata": {
        "id": "76SwEm_8I07t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
        "if gguf_files:\n",
        "  gguf_file = os.path.join(\"gguf_model\", gguf_files[1])\n",
        "  print(f\"Downloading {gguf_file}...\")\n",
        "  files.download(gguf_file)\n",
        "else:\n",
        "  print(\"No GGUF files found in the 'gguf_model' directory.\")"
      ],
      "metadata": {
        "id": "K4yihUxHME2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Run this command or login using huggingface-cli\n",
        "model.push_to_hub(\n",
        "    \"learn-abc/html-model-tinyllama-chat-bnb-4bit\",\n",
        "    tokenizer,\n",
        "    token=userdata.get(\"HF_ACCESS_TOKEN\"))\n",
        "'''"
      ],
      "metadata": {
        "id": "zWCzrCMzuCOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub -q"
      ],
      "metadata": {
        "id": "VBYSwBv6SngZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "FfiCHQq67VRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"learn-abc/html-model-tinyllama-chat-bnb-4bit\")"
      ],
      "metadata": {
        "id": "2jHt2Kio7gxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"learn-abc/html-model-tinyllama-chat-bnb-4bit\")"
      ],
      "metadata": {
        "id": "IaKgsMIl8Bpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Run this command or use huggingface-cli\n",
        "model.push_to_hub_gguf(\n",
        "    \"learn-abc/html-model-tinyllama-chat-bnb-4bit-gguf\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        "    token=userdata.get(\"HF_ACCESS_TOKEN\"))\n",
        "'''"
      ],
      "metadata": {
        "id": "hRVCSFkbu0WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_gguf(\"learn-abc/html-model-tinyllama-chat-bnb-4bit-gguf\")"
      ],
      "metadata": {
        "id": "OsyGd9Vk8LXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f19da2d"
      },
      "source": [
        "# Fine-tuned TinyLlama for JSON Extraction\n",
        "\n",
        "This repository contains a fine-tuned version of the `unsloth/tinyllama-chat-bnb-4bit` model, specifically trained for extracting product information from HTML snippets and outputting it in a JSON format.\n",
        "\n",
        "## Model Details\n",
        "\n",
        "- **Base Model:** `unsloth/tinyllama-chat-bnb-4bit`\n",
        "- **Fine-tuning Method:** LoRA (Low-Rank Adaptation)\n",
        "- **Trained on:** A custom dataset of HTML product snippets and their corresponding JSON representations.\n",
        "\n",
        "## Usage\n",
        "\n",
        "This model can be used for tasks involving structured data extraction from HTML content.\n",
        "\n",
        "### Loading the model\n",
        "\n",
        "You can load the model and tokenizer using the `transformers` library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d72332a"
      },
      "source": [
        "\"\"\"\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import json\n",
        "\n",
        "model_name = \"learn-abc/html-model-tinyllama-chat-bnb-4bit\" # Replace with your actual repo ID\n",
        "max_seq_length = 2048 # Or your chosen sequence length\n",
        "dtype = None # Auto detection\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Extract the product information:\\n<div class='product'><h2>iPad Air</h2><span class='price'>$1344</span><span class='category'>audio</span><span class='brand'>Dell</span></div>\"}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\") # Or \"cpu\" if not using GPU\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "qVE8o2FkBSCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aw_4P9CTBTaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f5905c3"
      },
      "source": [
        "# Fine-tuned TinyLlama for JSON Extraction (GGUF)\n",
        "\n",
        "This repository contains a fine-tuned version of the `unsloth/tinyllama-chat-bnb-4bit` model, specifically trained for extracting product information from HTML snippets and outputting it in a JSON format. This is the GGUF quantized version for use with tools like `llama.cpp` or other compatible inference engines.\n",
        "\n",
        "## Model Details\n",
        "\n",
        "- **Base Model:** `unsloth/tinyllama-chat-bnb-4bit`\n",
        "- **Fine-tuning Method:** LoRA (Low-Rank Adaptation)\n",
        "- **Quantization:** q4_k_m GGUF\n",
        "- **Trained on:** A custom dataset of HTML product snippets and their corresponding JSON representations.\n",
        "\n",
        "## Usage\n",
        "\n",
        "This model can be used for tasks involving structured data extraction from HTML content using GGUF compatible software.\n",
        "\n",
        "### Downloading and using the GGUF file\n",
        "\n",
        "You can download the GGUF file directly from the \"Files and versions\" tab on this repository page.\n",
        "\n",
        "To use this file with `llama.cpp`, you generally follow these steps:\n",
        "\n",
        "1.  **Download `llama.cpp`:** Clone the `llama.cpp` repository and build it. Follow the instructions in the `llama.cpp` README for building on your specific platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6000cdf"
      },
      "source": [
        "# Task\n",
        "Read the provided Jupyter Notebook and generate a detailed README.md file in markdown format for a GitHub repository of the fine-tuning project. The README should include sections covering the project description, setup instructions, dataset details, model fine-tuning process, usage instructions for inference, and links to the fine-tuned model and its GGUF version on the Hugging Face Hub. The notebook file is located at \"notebook.ipynb\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "496a3df6"
      },
      "source": [
        "## Analyze the notebook\n",
        "\n",
        "### Subtask:\n",
        "Go through each cell in the notebook to understand the different stages of the fine-tuning project, including dependencies, data loading and preprocessing, model loading and configuration, training process, and evaluation/testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40445b32"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to examine each cell in the notebook to understand the fine-tuning process. I will look at the code and the outputs of each cell to gather information about the steps involved, including dependencies, data handling, model specifics, training setup, and testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6f33b2"
      },
      "source": [
        "## Extract key information\n",
        "\n",
        "### Subtask:\n",
        "Identify and extract key information for the README, such as the base model used, the fine-tuning method, the dataset description, hyperparameters used for training, and instructions for running the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6383fc8b"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the key information from the notebook analysis and store it in a dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34a73322"
      },
      "source": [
        "## Structure the readme\n",
        "\n",
        "### Subtask:\n",
        "Create a clear and detailed structure for the README.md file, including sections like Project Title, Description, Setup, Dataset, Model Fine-tuning, Usage (for inference), and Hugging Face Links.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c14222cc"
      },
      "source": [
        "**Reasoning**:\n",
        "Outline the structure of the README.md file based on the requested sections and the extracted information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f360b14f"
      },
      "source": [
        "## Generate the readme content\n",
        "\n",
        "### Subtask:\n",
        "Write the content for each section of the README.md file in markdown format, incorporating the extracted information and providing clear explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e8d7289"
      },
      "source": [
        "**Reasoning**:\n",
        "Write the content for each section of the README.md file in markdown format using the extracted information from `readme_structure` and `readme_info`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b64b62"
      },
      "source": [
        "## Add hugging face links\n",
        "\n",
        "### Subtask:\n",
        "Include the links to the Hugging Face repository for both the fine-tuned model and the GGUF version.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b22d656a"
      },
      "source": [
        "## Format the readme\n",
        "\n",
        "### Subtask:\n",
        "Ensure the markdown is correctly formatted for readability on GitHub.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23e69cd7"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the generated readme_content string to ensure correct markdown syntax and readability on GitHub.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d83fa8"
      },
      "source": [
        "## Present the readme\n",
        "\n",
        "### Subtask:\n",
        "Provide the complete markdown code for the README.md file in a markdown cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1db84754"
      },
      "source": [
        "**Reasoning**:\n",
        "The README content has been generated and formatted. The final step is to provide this content in a markdown cell as requested by the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33259dc5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed due to a `SyntaxError` in the markdown content within the code block. The error message points to an issue with a decimal literal, likely caused by including markdown directly in a Python string without proper escaping or using raw strings. I need to correctly format the markdown content as a raw string to avoid this issue and ensure the markdown is rendered correctly in the new cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da527fad"
      },
      "source": [
        "**Reasoning**:\n",
        "The markdown content for the README has been successfully generated and displayed as a string. The subtask requires placing this content into a markdown cell. While the previous command displayed the string, it didn't create a markdown cell with the content. The next step is to create a markdown cell and put the `readme_markdown` content inside it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b73aa67"
      },
      "source": [
        "**Reasoning**:\n",
        "The markdown content for the README is now stored in the `readme_markdown` variable and has been successfully displayed as a string. To fulfill the subtask of providing the complete markdown code in a markdown cell, I need to create a markdown cell and place the content of `readme_markdown` inside it. The `display()` function with a markdown string as input will achieve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb6f0c8d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to provide the complete markdown code for the README.md file in a markdown cell. The `readme_markdown` variable contains the correct markdown content. The `display()` function, when given a string that starts and ends with triple backticks and the word \"markdown\", will render the content as a markdown cell in the notebook. This will fulfill the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81443a26"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to provide the complete markdown code for the README.md file in a markdown cell. I have the markdown content in the `readme_markdown` variable. Using `display()` with this variable will create a markdown cell with the content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04379e31"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to provide the complete markdown code for the README.md file in a markdown cell. I have the markdown content stored in the `readme_markdown` variable. Using the `display` function on this variable will create a markdown cell in the notebook and render the content, fulfilling the subtask.\n",
        "\n"
      ]
    }
  ]
}